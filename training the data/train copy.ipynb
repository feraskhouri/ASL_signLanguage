{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import mediapipe\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from skimage.transform import resize\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import animation, rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "data = pd.read_csv(\"D:\\\\finalProject\\\\landmarks_all.csv\")\n",
    "X = data.drop(columns=[\"label\"]).values\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landmark_1</th>\n",
       "      <th>landmark_2</th>\n",
       "      <th>landmark_3</th>\n",
       "      <th>landmark_4</th>\n",
       "      <th>landmark_5</th>\n",
       "      <th>landmark_6</th>\n",
       "      <th>landmark_7</th>\n",
       "      <th>landmark_8</th>\n",
       "      <th>landmark_9</th>\n",
       "      <th>landmark_10</th>\n",
       "      <th>...</th>\n",
       "      <th>landmark_91</th>\n",
       "      <th>landmark_92</th>\n",
       "      <th>landmark_93</th>\n",
       "      <th>landmark_94</th>\n",
       "      <th>landmark_95</th>\n",
       "      <th>landmark_96</th>\n",
       "      <th>landmark_97</th>\n",
       "      <th>landmark_98</th>\n",
       "      <th>landmark_99</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.474078</td>\n",
       "      <td>0.255586</td>\n",
       "      <td>-0.845679</td>\n",
       "      <td>0.492901</td>\n",
       "      <td>0.215314</td>\n",
       "      <td>-0.802644</td>\n",
       "      <td>0.505039</td>\n",
       "      <td>0.216036</td>\n",
       "      <td>-0.803109</td>\n",
       "      <td>0.515049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356639</td>\n",
       "      <td>2.313848</td>\n",
       "      <td>0.377244</td>\n",
       "      <td>0.502491</td>\n",
       "      <td>2.394536</td>\n",
       "      <td>-0.002479</td>\n",
       "      <td>0.381498</td>\n",
       "      <td>2.377426</td>\n",
       "      <td>-0.099456</td>\n",
       "      <td>a-abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.475068</td>\n",
       "      <td>0.255527</td>\n",
       "      <td>-1.028458</td>\n",
       "      <td>0.493540</td>\n",
       "      <td>0.215332</td>\n",
       "      <td>-0.985430</td>\n",
       "      <td>0.505772</td>\n",
       "      <td>0.216051</td>\n",
       "      <td>-0.985767</td>\n",
       "      <td>0.515587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358301</td>\n",
       "      <td>2.340979</td>\n",
       "      <td>0.728185</td>\n",
       "      <td>0.504685</td>\n",
       "      <td>2.416029</td>\n",
       "      <td>0.110209</td>\n",
       "      <td>0.381616</td>\n",
       "      <td>2.409318</td>\n",
       "      <td>0.237820</td>\n",
       "      <td>a-abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.475504</td>\n",
       "      <td>0.255363</td>\n",
       "      <td>-1.009970</td>\n",
       "      <td>0.493964</td>\n",
       "      <td>0.215333</td>\n",
       "      <td>-0.965716</td>\n",
       "      <td>0.506334</td>\n",
       "      <td>0.216048</td>\n",
       "      <td>-0.966104</td>\n",
       "      <td>0.516079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358707</td>\n",
       "      <td>2.358446</td>\n",
       "      <td>0.656555</td>\n",
       "      <td>0.504669</td>\n",
       "      <td>2.430796</td>\n",
       "      <td>0.102040</td>\n",
       "      <td>0.381559</td>\n",
       "      <td>2.424881</td>\n",
       "      <td>0.166357</td>\n",
       "      <td>a-abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.475864</td>\n",
       "      <td>0.255349</td>\n",
       "      <td>-1.026346</td>\n",
       "      <td>0.494418</td>\n",
       "      <td>0.215733</td>\n",
       "      <td>-0.981857</td>\n",
       "      <td>0.506879</td>\n",
       "      <td>0.216520</td>\n",
       "      <td>-0.982215</td>\n",
       "      <td>0.516564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362065</td>\n",
       "      <td>2.366050</td>\n",
       "      <td>0.698646</td>\n",
       "      <td>0.508891</td>\n",
       "      <td>2.437438</td>\n",
       "      <td>0.104160</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>2.432841</td>\n",
       "      <td>0.210205</td>\n",
       "      <td>a-abc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.476150</td>\n",
       "      <td>0.255345</td>\n",
       "      <td>-1.036166</td>\n",
       "      <td>0.494780</td>\n",
       "      <td>0.216127</td>\n",
       "      <td>-0.992960</td>\n",
       "      <td>0.507264</td>\n",
       "      <td>0.217017</td>\n",
       "      <td>-0.993278</td>\n",
       "      <td>0.516975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373359</td>\n",
       "      <td>2.372369</td>\n",
       "      <td>0.749761</td>\n",
       "      <td>0.519158</td>\n",
       "      <td>2.440346</td>\n",
       "      <td>0.085969</td>\n",
       "      <td>0.392524</td>\n",
       "      <td>2.440275</td>\n",
       "      <td>0.263123</td>\n",
       "      <td>a-abc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   landmark_1  landmark_2  landmark_3  landmark_4  landmark_5  landmark_6  \\\n",
       "0    0.474078    0.255586   -0.845679    0.492901    0.215314   -0.802644   \n",
       "1    0.475068    0.255527   -1.028458    0.493540    0.215332   -0.985430   \n",
       "2    0.475504    0.255363   -1.009970    0.493964    0.215333   -0.965716   \n",
       "3    0.475864    0.255349   -1.026346    0.494418    0.215733   -0.981857   \n",
       "4    0.476150    0.255345   -1.036166    0.494780    0.216127   -0.992960   \n",
       "\n",
       "   landmark_7  landmark_8  landmark_9  landmark_10  ...  landmark_91  \\\n",
       "0    0.505039    0.216036   -0.803109     0.515049  ...     0.356639   \n",
       "1    0.505772    0.216051   -0.985767     0.515587  ...     0.358301   \n",
       "2    0.506334    0.216048   -0.966104     0.516079  ...     0.358707   \n",
       "3    0.506879    0.216520   -0.982215     0.516564  ...     0.362065   \n",
       "4    0.507264    0.217017   -0.993278     0.516975  ...     0.373359   \n",
       "\n",
       "   landmark_92  landmark_93  landmark_94  landmark_95  landmark_96  \\\n",
       "0     2.313848     0.377244     0.502491     2.394536    -0.002479   \n",
       "1     2.340979     0.728185     0.504685     2.416029     0.110209   \n",
       "2     2.358446     0.656555     0.504669     2.430796     0.102040   \n",
       "3     2.366050     0.698646     0.508891     2.437438     0.104160   \n",
       "4     2.372369     0.749761     0.519158     2.440346     0.085969   \n",
       "\n",
       "   landmark_97  landmark_98  landmark_99  label  \n",
       "0     0.381498     2.377426    -0.099456  a-abc  \n",
       "1     0.381616     2.409318     0.237820  a-abc  \n",
       "2     0.381559     2.424881     0.166357  a-abc  \n",
       "3     0.384000     2.432841     0.210205  a-abc  \n",
       "4     0.392524     2.440275     0.263123  a-abc  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove classes with fewer than 2 samples\n",
    "class_counts = np.bincount(y_encoded)\n",
    "valid_classes = np.where(class_counts > 1)[0]\n",
    "valid_indices = np.isin(y_encoded, valid_classes)\n",
    "X_valid = X[valid_indices]\n",
    "y_valid = y_encoded[valid_indices]\n",
    "\n",
    "# Stratified split for balanced training and validation sets\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, val_index in split.split(X_valid, y_valid):\n",
    "    X_train, X_val = X_valid[train_index], X_valid[val_index]\n",
    "    y_train, y_val = y_valid[train_index], y_valid[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input for LSTM if necessary\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\\\\\finalProject\\\\\\\\training the data\\\\\\\\label_encoder.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the label encoder for future use\n",
    "joblib.dump(label_encoder, r\"D:\\\\finalProject\\\\training the data\\\\label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f1930\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized to add `training` variable\n",
    "# Reference: https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target, training):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n",
    "        return ffn_out_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized to add edit_dist metric and training variable.\n",
    "# Reference:\n",
    "# https://www.kaggle.com/code/irohith/aslfr-transformer/notebook\n",
    "# https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=99,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=60,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_metric = keras.metrics.Mean(name=\"edit_dist\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target, training):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        x = self.encoder(source, training)\n",
    "        y = self.decode(x, target, training)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        \n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
    "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Computes the Levenshtein distance between sequences since the evaluation\n",
    "        # metric for this contest is the normalized total levenshtein distance.\n",
    "        edit_dist = tf.edit_distance(tf.sparse.from_dense(target), \n",
    "                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n",
    "        edit_dist = tf.reduce_mean(edit_dist)\n",
    "        self.acc_metric.update_state(edit_dist)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):        \n",
    "        source = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        \n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
    "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        # Computes the Levenshtein distance between sequences since the evaluation\n",
    "        # metric for this contest is the normalized total levenshtein distance.\n",
    "        edit_dist = tf.edit_distance(tf.sparse.from_dense(target), \n",
    "                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n",
    "        edit_dist = tf.reduce_mean(edit_dist)\n",
    "        self.acc_metric.update_state(edit_dist)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source, training = False)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input, training = False)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = logits[:, -1][..., tf.newaxis]\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, batch, idx_to_token, target_start_token_idx=60, target_end_token_idx=61\n",
    "    ):\n",
    "        \"\"\"Displays a batch of outputs after every 4 epoch\n",
    "\n",
    "        Args:\n",
    "            batch: A test batch\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 4 != 0:\n",
    "            return\n",
    "        source = self.batch[0]\n",
    "        target = self.batch[1].numpy()\n",
    "        bs = tf.shape(source)[0]\n",
    "        preds = self.model.generate(source, self.target_start_token_idx)\n",
    "        preds = preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += self.idx_to_char[idx]\n",
    "                if idx == self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:     {target_text.replace('-','')}\")\n",
    "            print(f\"prediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m y_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(y_val, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[62], line 76\u001b[0m, in \u001b[0;36mTransformer.train_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     74\u001b[0m dec_target \u001b[38;5;241m=\u001b[39m target[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 76\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     one_hot \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mone_hot(dec_target, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[0;32m     78\u001b[0m     mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlogical_not(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mequal(dec_target, pad_token_idx))\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\inspect.py:3242\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3239\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\inspect.py:3157\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3155\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required\u001b[39m\u001b[38;5;132;01m{argtype}\u001b[39;00m\u001b[38;5;124m argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   3156\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname, argtype\u001b[38;5;241m=\u001b[39margtype)\n\u001b[1;32m-> 3157\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[0;32m   3160\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: missing a required argument: 'training'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the vocabulary\n",
    "with open(\"D:\\\\finalProject\\\\norm.json\", \"r\") as file:\n",
    "    char_to_num = json.load(file)\n",
    "\n",
    "# Reverse mapping from index to character\n",
    "idx_to_char = {v: k for k, v in char_to_num.items()}\n",
    "\n",
    "# Define pad token index (set based on your data structure)\n",
    "pad_token_idx = 0\n",
    "\n",
    "# Define model instance using the existing Transformer class\n",
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=4,\n",
    "    num_feed_forward=400,\n",
    "    source_maxlen=99,\n",
    "    target_maxlen=64,\n",
    "    num_layers_enc=2,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=len(char_to_num) , # Ensure num_classes matches vocabulary size\n",
    ")\n",
    "\n",
    "# Loss and optimizer setup\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
    "optimizer = keras.optimizers.Adam(0.0001)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# Reshape y_train and y_val to ensure at least 2 dimensions\n",
    "y_train = np.expand_dims(y_train, axis=-1)\n",
    "y_val = np.expand_dims(y_val, axis=-1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=13, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[19], line 76\u001b[0m, in \u001b[0;36mTransformer.train_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     74\u001b[0m dec_target \u001b[38;5;241m=\u001b[39m target[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 76\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     one_hot \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mone_hot(dec_target, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[0;32m     78\u001b[0m     mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlogical_not(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mequal(dec_target, pad_token_idx))\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\inspect.py:3242\u001b[0m, in \u001b[0;36mSignature.bind\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[0;32m   3239\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\inspect.py:3157\u001b[0m, in \u001b[0;36mSignature._bind\u001b[1;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[0;32m   3155\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required\u001b[39m\u001b[38;5;132;01m{argtype}\u001b[39;00m\u001b[38;5;124m argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   3156\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname, argtype\u001b[38;5;241m=\u001b[39margtype)\n\u001b[1;32m-> 3157\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[0;32m   3160\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: missing a required argument: 'training'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"D:\\\\finalProject\\\\training the data\\\\asl_sign_model3.h5\")\n",
    "\n",
    "print(\"Model trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ab22bc87d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
